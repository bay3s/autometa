from typing import Tuple, Any, List, Optional
import numpy as np

import gym
from gym.utils import EzPickle, seeding
from gym import spaces

from autometa.envs.base_randomized_env import BaseRandomizedEnv
from autometa.randomization.randomization_parameter import RandomizationParameter
from autometa.randomization.randomization_bound_type import RandomizationBoundType
from autometa.randomization.randomization_bound import RandomizationBound


class PointNavigationEnv(EzPickle, BaseRandomizedEnv):
    RANDOMIZABLE_PARAMETERS = [
        RandomizationParameter(
            name="x_position",
            lower_bound=RandomizationBound(
                type=RandomizationBoundType.LOWER_BOUND,
                value=-0.05,
                min_value=-3.0,
                max_value=-0.05,
            ),
            upper_bound=RandomizationBound(
                type=RandomizationBoundType.UPPER_BOUND,
                value=0.05,
                min_value=0.05,
                max_value=3.0,
            ),
            delta=0.05,
        ),
        RandomizationParameter(
            name="y_position",
            lower_bound=RandomizationBound(
                type=RandomizationBoundType.LOWER_BOUND,
                value=-0.05,
                min_value=-3.0,
                max_value=-0.05,
            ),
            upper_bound=RandomizationBound(
                type=RandomizationBoundType.UPPER_BOUND,
                value=0.05,
                min_value=0.05,
                max_value=3.0,
            ),
            delta=0.05,
        ),
    ]

    def __init__(
        self,
        episode_length: int,
        randomizable_parameters: List[RandomizationParameter] = RANDOMIZABLE_PARAMETERS,
        auto_reset: bool = True,
        seed: int = None,
    ):
        """
        2D navigation problems, as described in [1]. The code is adapted from https://github.com/cbfinn/maml_rl/

        At each time step, the 2D agent takes an action (its velocity, clipped in [-0.1, 0.1]), and receives a penalty
        equal to its L2 distance to the goal position (i.e. the reward is `-distance`).

        The 2D navigation tasks are generated by sampling goal positions from the uniform distribution on [-0.5, 0.5]^2.

        [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic Meta-Learning for Fast Adaptation of Deep
        Networks", 2017 (https://arxiv.org/abs/1703.03400)

        Args:
            episode_length (int): Max number of episode steps.
            randomizable_parameters (List[RandomizationParameter]): List of randomized parameters and specs.
            auto_reset (bool): Auto-reset the environment after completion.
            seed (int): Random seed for sampling.
        """
        EzPickle.__init__(self)
        BaseRandomizedEnv.__init__(self, seed)

        self.viewer = None
        self._episode_length = episode_length
        self._episode_reward = 0.0
        self._elapsed_steps = 0
        self._auto_reset = auto_reset

        self._num_dimensions = 2
        self._start_state = np.zeros(self._num_dimensions, dtype=np.float32)

        # params
        self._randomized_parameters = self._init_params(randomizable_parameters)

        # sampled
        self._current_state = None
        self._target_state = None

        # spaces
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(2,))
        self.action_space = spaces.Box(low=-0.1, high=0.1, shape=(2,))
        pass

    @staticmethod
    def _init_params(params: List[RandomizationParameter]) -> dict:
        """
        Convert a list of parameters to dict.

        Args:
            params (List[RandomizationParameter]): A list of randomized parameters.

        Returns:
            dict
        """
        randomized = dict()
        for param in params:
            randomized[param.name] = param

        return randomized

    def randomizable_parameters(self) -> List[RandomizationParameter]:
        """
        Return a list of randomized parameters.

        Returns:
            List[RandomizedParameter]
        """
        return self.RANDOMIZABLE_PARAMETERS

    def randomized_parameter(self, param_name: str) -> RandomizationParameter:
        """
        Return the randomized parameter.

        Args:
            param_name (str): Name of the parameter to return.

        Returns:
            RandomizationParameter
        """
        return self._randomized_parameters[param_name]

    def sample_task(self, task: dict = None) -> None:
        """
        Sample a new goal position for the navigation task

        Returns:
            None
        """
        if task is None:
            task = dict()

            x_param = self.randomized_parameter("x_position")
            task["x_position"] = self.np_random.uniform(
                x_param.lower_bound.min_value, x_param.upper_bound.max_value
            )

            y_param = self.randomized_parameter("y_position")
            task["y_position"] = self.np_random.uniform(
                y_param.lower_bound.min_value, y_param.upper_bound.max_value
            )

        self._target_state = np.concatenate(
            [
                [task["x_position"]],
                [task["y_position"]],
            ]
        )

    def reset(
        self, *, seed: Optional[int] = None, options: Optional[dict] = None
    ) -> Tuple:
        """
        Resets the environment and returns the current observation.

        Args:
            seed (int): Random seed.
            options (dict): Additional options.

        Returns:
            Tuple
        """
        if seed is not None:
            self.np_random, seed = seeding.np_random(seed)

        self._current_state = self._start_state
        self._elapsed_steps = 0
        self._episode_reward = 0.0

        return self._current_state, {}

    def step(self, action: np.ndarray) -> Tuple:
        """
        Take a step in the environment and return the corresponding observation, action, reward, plus additional
        info.

        Args:
            action (np.ndarray): Action to be taken in the environment.

        Returns:
            Tuple
        """
        if self._target_state is None:
            raise ValueError("`sample_task` required before calling `step`.")

        self._elapsed_steps += 1
        action = np.clip(action, -0.1, 0.1)

        assert self.action_space.contains(action)
        self._current_state = self._current_state + action

        x_dist = self._current_state[0] - self._target_state[0]
        y_dist = self._current_state[1] - self._target_state[1]
        reward = -np.sqrt(x_dist**2 + y_dist**2)

        terminated = (np.abs(x_dist) < 0.01) and (np.abs(y_dist) < 0.01)
        self._episode_reward += reward

        truncated = self.elapsed_steps == self.max_episode_steps
        done = truncated or terminated

        info = dict()
        info["sampled_task"] = self._target_state

        if done:
            info["episode"] = {}
            info["episode"]["r"] = self._episode_reward

            if self._auto_reset:
                self._current_state, _ = self.reset()
                pass

        return self._current_state, reward, terminated, truncated, info

    @property
    def observation_space(self) -> gym.Space:
        """
        Returns the observation space of the environment.

        Returns:
          gym.Space
        """
        return self._observation_space

    @observation_space.setter
    def observation_space(self, value: Any) -> None:
        """
        Set the observation space for the environment.

        Returns:
          gym.Space
        """
        self._observation_space = value

    @property
    def action_space(self) -> gym.Space:
        """
        Returns the action space

        Returns:
          gym.Space
        """
        return self._action_space

    @action_space.setter
    def action_space(self, value: Any) -> None:
        """
        Set the action space for the environment.

        Returns:
            gym.Space
        """
        self._action_space = value

    def get_spaces(self) -> Tuple[gym.Space, gym.Space]:
        """
        Get the observation space and the action space.

        Returns:
            Tuple
        """
        return self._observation_space, self._action_space

    @property
    def elapsed_steps(self) -> int:
        """
        Returns the elapsed number of episode steps in the environment.

        Returns:
          int
        """
        return self._elapsed_steps

    @property
    def max_episode_steps(self) -> int:
        """
        Returns the maximum number of episode steps in the environment.

        Returns:
          int
        """
        return self._episode_length

    def render(self, mode: str = "human") -> None:
        """
        Render the environment given the render mode.

        Args:
          mode (str): Mode in which to render the environment.

        Returns:
          None
        """
        pass

    def close(self) -> None:
        """
        Close the environment.

        Returns:
          None
        """
        pass
